{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECqwODcqCaXb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Add L2 regularization and dropout layers to the model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load and preprocess the data\n",
        "data = pd.read_csv('dataset.csv')"
      ],
      "metadata": {
        "id": "qpJyNmTeC_wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "wV3IzcP66IgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the column names to inspect them\n",
        "print(\"Columns in the dataset:\", data.columns.tolist())"
      ],
      "metadata": {
        "id": "osb0qVGxDfNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_features = ['cell_migration', 'cell_invasion', 'cell_growth', 'wound_clousure', 'protein_expression', 'colonization', 'average_tumor_volume', 'cell_poliferation_G0-G1phase', 'cell_proliferation_Sphase', 'cell_proliferation_G2-Mphase', 'apoptosis', 'mrna_expression_levels']"
      ],
      "metadata": {
        "id": "PsACCqUdDMKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop any rows with missing values (optional)\n",
        "data = data.dropna()"
      ],
      "metadata": {
        "id": "lk6JNyXVDOs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and targets (train on all features)\n",
        "X = data[all_features]\n",
        "y = data[all_features]"
      ],
      "metadata": {
        "id": "mfwlKXAhDQ1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "rnbrfOVUDUpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "h92d3rj-ER8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_dim, output_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(32, input_dim=input_dim, activation='relu', kernel_regularizer=l2(0.001)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(output_dim, activation='linear'))  # Output layer for regression (multiple outputs)\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='mse')\n",
        "    return model\n",
        "\n",
        "# Early stopping to avoid overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "dr4h5gJuET3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "input_dim = X_train_scaled.shape[1]  # Number of input features (all features)\n",
        "output_dim = y_train.shape[1]  # Number of target variables (all features)\n",
        "model = build_model(input_dim, output_dim)"
      ],
      "metadata": {
        "id": "-AO68pXHEVx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=500, batch_size=32, verbose=1, callbacks=[early_stopping])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "t_m_rsNCEXoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on the test set\n",
        "test_loss = model.evaluate(X_test_scaled, y_test)\n",
        "print(f'Test Loss: {test_loss}')"
      ],
      "metadata": {
        "id": "K76eJu8YNyk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ML89u4emH8h6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model performance on the test set\n",
        "test_loss = model.evaluate(X_test_scaled, y_test)\n",
        "print(f'Test Loss: {test_loss}')\n",
        "\n",
        "# Compare with training loss\n",
        "train_loss = history.history['loss'][-1]\n",
        "print(f'Training Loss: {train_loss}')\n",
        "\n",
        "if test_loss > train_loss:\n",
        "    print(\"The model is likely overfitting.\")\n",
        "else:\n",
        "    print(\"The model does not appear to be overfitting.\")"
      ],
      "metadata": {
        "id": "9jVOi_sKIHM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Flexible Prediction Interface\n",
        "def predict_output(selected_input_features, target_feature):\n",
        "    # Ensure the input features are a subset of the all_features list\n",
        "    assert all(feature in all_features for feature in selected_input_features), \"Invalid input features\"\n",
        "    assert target_feature in all_features, \"Invalid target feature\"\n",
        "\n",
        "    # Extract only the selected input features and target feature from the training set\n",
        "    X_selected = data[selected_input_features]\n",
        "    y_selected = data[[target_feature]]\n",
        "\n",
        "    # Standardize the selected input features\n",
        "    scaler_selected = StandardScaler()\n",
        "    X_selected_scaled = scaler_selected.fit_transform(X_selected)\n",
        "\n",
        "    # Build a model for the selected features\n",
        "    selected_input_dim = len(selected_input_features)\n",
        "    selected_output_dim = 1  # Predicting a single target\n",
        "    selected_model = build_model(selected_input_dim, selected_output_dim)\n",
        "\n",
        "    # Train the model on the selected input features\n",
        "    X_train_sel, X_test_sel, y_train_sel, y_test_sel = train_test_split(X_selected_scaled, y_selected, test_size=0.2, random_state=42)\n",
        "    selected_model.fit(X_train_sel, y_train_sel, validation_split=0.2, epochs=100, batch_size=32, verbose=1)\n",
        "\n",
        "    # Predict on new data\n",
        "    def predict(selected_values):\n",
        "        selected_values_scaled = scaler_selected.transform([selected_values])\n",
        "        prediction = selected_model.predict(selected_values_scaled)\n",
        "        return prediction[0][0]\n",
        "\n",
        "    return predict"
      ],
      "metadata": {
        "id": "A163Bd9jEcPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_input_features = ['cell_migration', 'cell_invasion']\n",
        "target_feature = 'wound_clousure'"
      ],
      "metadata": {
        "id": "WWxICVeSEsS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert all(feature in data.columns for feature in selected_input_features), \"One or more selected input features do not exist in the dataset.\"\n",
        "assert target_feature in data.columns, \"Target feature does not exist in the dataset.\""
      ],
      "metadata": {
        "id": "x9102jgqEvNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_fn = predict_output(selected_input_features, target_feature)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "S8OK5td7FVLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_values = [0.15, 0.25]\n",
        "predicted_value = predict_fn(input_values)\n",
        "print(f'Predicted {target_feature} value: {predicted_value}')"
      ],
      "metadata": {
        "id": "gpdgd04uFq9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate Mean Absolute Error\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "\n",
        "# Calculate R-squared\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f'R-squared (R²): {r2}')"
      ],
      "metadata": {
        "id": "9smO9AQmFuNy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}